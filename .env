LOCAL_MODEL_CACHE_DIR= # location on your host machine for the huggingface cache
LLAMA_TOKEN= # if you are wanting to run llama2 from huggingface you will need a token
MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
QUANTIZATION=bitsandbytes-nf4
MAX_PREFILL_TOKENS=3072
MAX_TOTAL_TOKENS=4098
MAX_INPUT_LENGTH=3000